A. Linear Regression (Predicting Salary from Experience)

import pandas as pd
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Sample data
data = {'Experience': [1.1, 1.3, 1.5, 2.0, 2.2, 2.9, 3.0, 3.2, 3.2, 3.7],
        'Salary': [39343, 46205, 37731, 43525, 39891, 56642, 60150, 54445, 64445, 57189]}
df = pd.DataFrame(data)

# Linear Regression
X = df[['Experience']]
y = df['Salary']
model = LinearRegression()
model.fit(X, y)

# Prediction
experience = [[4.0]]
predicted_salary = model.predict(experience)
print("Predicted Salary for 4 years experience:", predicted_salary)

# Visualization
plt.scatter(X, y, color='blue')
plt.plot(X, model.predict(X), color='red')
plt.xlabel("Years of Experience")
plt.ylabel("Salary")
plt.title("Linear Regression: Salary vs Experience")
plt.show()

=============================================================================================================================================

B. VLOOKUP in Excel

Steps:

Open the dataset in Excel.

Use VLOOKUP formula:

=VLOOKUP("A002", A2:D10, 2, FALSE) → Part Name for “A002”

=VLOOKUP("Ball Joint", B2:D10, 3, FALSE) → Supplier ID for “Ball Joint”

=VLOOKUP("Muffler", B2:D10, 3, FALSE) → Part Price for “Muffler”

=VLOOKUP("A008", A2:D10, 4, FALSE) → Status for “A008”
=============================================================================================================================================

A. One-Sample T-Test
Formulate Hypotheses:
Null Hypothesis (H₀): μ = 70 (Mean student score is 70)
Alternative Hypothesis (H₁): μ ≠ 70 (Mean student score is different from 70)

from scipy import stats

scores = [72, 88, 64, 74, 67, 79, 85, 75, 89, 77]
hypothesized_mean = 70

# Perform one-sample t-test
t_stat, p_value = stats.ttest_1samp(scores, hypothesized_mean)

print(f"T-statistic: {t_stat:.3f}")
print(f"P-value: {p_value:.4f}")

# Interpret
alpha = 0.05
if p_value < alpha:
    print("Reject Null Hypothesis: Mean is significantly different from 70.")
else:
    print("Fail to Reject Null Hypothesis: No significant difference from 70.")


=============================================================================================================================================
 B. Feature Scaling on Boston Housing Dataset

from sklearn.datasets import load_boston
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import pandas as pd

# Load dataset
boston = load_boston()
df = pd.DataFrame(boston.data, columns=boston.feature_names)

# Standardization
standard_scaler = StandardScaler()
df_standardized = pd.DataFrame(standard_scaler.fit_transform(df), columns=df.columns)

# Normalization
minmax_scaler = MinMaxScaler()
df_normalized = pd.DataFrame(minmax_scaler.fit_transform(df), columns=df.columns)

# Show samples
print("Standardized Data (first 5 rows):")
print(df_standardized.head())

print("\nNormalized Data (first 5 rows):")
print(df_normalized.head())

=============================================================================================================================================
A. Logistic Regression on Iris Dataset (Binary Classification)

from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# Load Iris dataset
iris = load_iris()
X = iris.data
y = (iris.target == 0).astype(int)  # Binary: Setosa (1), Others (0)

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Logistic Regression Model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

=============================================================================================================================================
 B. Pivot Table & Pivot Chart in Excel

Steps in Excel:
Insert Pivot Table:

Select your dataset

Go to Insert > PivotTable

Choose New Worksheet

Total Sales:

Drag Sales to Values → it will show Sum of Sales

Sum of Sales by Color:

Drag Color to Rows

Drag Sales to Values → Sum of Sales for each color

Sum of Units:

Drag Units to Values → it will show Sum of Units

Region-wise Sales & Units:

Drag Region to Rows

Drag Sales and Units to Values

Insert PivotChart:

With Pivot Table selected → Go to Insert > PivotChart

Choose a column or bar chart

=============================================================================================================================================
Part 1: Decision Tree on Titanic Dataset

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, export_text
from sklearn.metrics import accuracy_score
import seaborn as sns

# Load Titanic dataset
titanic = sns.load_dataset("titanic")

# Preprocess
titanic = titanic[['survived', 'pclass', 'sex', 'age']]
titanic.dropna(inplace=True)
titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})

# Features and target
X = titanic[['pclass', 'sex', 'age']]
y = titanic['survived']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train Decision Tree
clf = DecisionTreeClassifier(max_depth=3)
clf.fit(X_train, y_train)

# Predictions
y_pred = clf.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))

# Interpret decision rules
rules = export_text(clf, feature_names=list(X.columns))
print("\nDecision Tree Rules:\n", rules)

=============================================================================================================================================
Part 2: Linear Regression (Height → Weight)

import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Sample Data
data = pd.DataFrame({
    'Height': [150, 160, 170, 180, 190],
    'Weight': [45, 55, 65, 75, 85]
})

# Linear Regression
X = data[['Height']]
y = data['Weight']
model = LinearRegression()
model.fit(X, y)

# Prediction
print("Predicted weight for 175 cm:", model.predict([[175]])[0])

# Plot
plt.scatter(X, y, color='blue')
plt.plot(X, model.predict(X), color='red')
plt.xlabel("Height")
plt.ylabel("Weight")
plt.title("Linear Regression: Height vs Weight")
plt.grid()
plt.show()

=============================================================================================================================================
B. CSV File + Transformation Functions
Sample Data (cars.csv):
Model,Make,BuyPrice,SellPrice
X1,BMW,3500,4500
Civic,Honda,2800,4000
Corolla,Toyota,3100,3900
City,Honda,4000,4800

--on notepad save as a cars.csv ☝️---
import pandas as pd

# Read CSV
df = pd.read_csv('cars.csv')

# Filter Buy Price >= 3000
filtered = df[df['BuyPrice'] >= 3000]
print("Filtered Records:\n", filtered)

# Sort ascending by BuyPrice
sorted_df = df.sort_values(by='BuyPrice')
print("\nSorted Data:\n", sorted_df)

# Group by Model
grouped = df.groupby('Model').mean(numeric_only=True)
print("\nGrouped by Model:\n", grouped)
=============================================================================================================================================
A. CSV File + Transformation Functions
You are asked to:

Create and read a CSV file into a DataFrame

Perform the following using Python/R:

Display records with Sell Price > 4000
-------------------------------------
Model,Make,BuyPrice,SellPrice
Civic,Honda,2800,4000
City,Honda,4200,5000
X1,BMW,3900,4500
Corolla,Toyota,3100,4200
Accord,Honda,3000,3800
--on notepad save as a cars.csv ☝️---
import pandas as pd

# Step 1: Read CSV
df = pd.read_csv("cars.csv")

# Step 2: Filter SellPrice > 4000
filtered = df[df['SellPrice'] > 4000]
print("Cars with Sell Price > 4000:\n", filtered)

# Step 3: Sort in ascending order (by SellPrice)
sorted_df = df.sort_values(by='SellPrice')
print("\nSorted Data:\n", sorted_df)

# Step 4: Group by Make
grouped = df.groupby('Make').mean(numeric_only=True)
print("\nGrouped by Make:\n", grouped)

=============================================================================================================================================

B. PCA on Iris Dataset

from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Load Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Apply PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Create DataFrame for plotting
pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])
pca_df['target'] = y

# Visualize
plt.figure(figsize=(8, 6))
sns.scatterplot(x='PC1', y='PC2', hue='target', palette='Set2', data=pca_df)
plt.title('PCA of Iris Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid()
plt.show()

=============================================================================================================================================

A. Linear Regression on Iris Dataset
Task: Predict petal.width based on petal.length

from sklearn.datasets import load_iris
from sklearn.linear_model import LinearRegression
import pandas as pd
import matplotlib.pyplot as plt

# Load dataset
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)

# Use petal length and petal width
X = df[['petal length (cm)']]
y = df['petal width (cm)']

# Train model
model = LinearRegression()
model.fit(X, y)

# Prediction example
pred = model.predict([[4.5]])
print(f"Predicted petal width for petal length 4.5 cm: {pred[0]:.2f} cm")

# Visualization
plt.scatter(X, y, color='blue')
plt.plot(X, model.predict(X), color='red')
plt.xlabel("Petal Length (cm)")
plt.ylabel("Petal Width (cm)")
plt.title("Linear Regression: Petal Width vs Petal Length")
plt.grid()
plt.show()

=============================================================================================================================================
 
B. What-If Analysis using Goal Seek in Excel

🔸 Steps in Excel:
1.Create a formula:
=AVERAGE(B1:B5) → where B1 to B5 are the marks

2.Select the cell with the Final Grade.

3.Go to:
Data → What-If Analysis → Goal Seek

Set:

4.Set cell: Final Grade cell (e.g., B6)

To value: 72

5.By changing cell: Final Exam marks cell (e.g., B5)

Click OK → Excel will calculate the required marks in Final Exam

=============================================================================================================================================

A. Data Pre-processing on CSV File
sample data
Area,Bedrooms,Price
1200,2,250000
1350,3,275000
NaN,3,300000
1600,,320000
1700,3,5000000
1400,2,NaN
---Change according to your slip data and save as c.csv-----------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load CSV
df = pd.read_csv("c.csv")

print("Original Data:")
print(df)

# Handle missing values
df['Area'].fillna(df['Area'].mean(), inplace=True)
df['Bedrooms'].fillna(df['Bedrooms'].mode()[0], inplace=True)
df['Price'].fillna(df['Price'].median(), inplace=True)

print("\nAfter handling missing values:")
print(df)

# Detect outliers using boxplot
plt.figure(figsize=(8, 4))
sns.boxplot(x=df['Price'])
plt.title("Price Outliers")
plt.show()

# Remove outliers (e.g., z-score or IQR method)
Q1 = df['Price'].quantile(0.25)
Q3 = df['Price'].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR

df_clean = df[(df['Price'] >= lower) & (df['Price'] <= upper)]
print("\nAfter removing outliers:")
print(df_clean)

=============================================================================================================================================

B. Multiple Linear Regression on Housing Dataset

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error

# Using cleaned data from Part A
X = df_clean[['Area', 'Bedrooms']]
y = df_clean['Price']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation
print("R² Score:", r2_score(y_test, y_pred))
print("MSE:", mean_squared_error(y_test, y_pred))

# Predict sample
sample = pd.DataFrame({'Area': [1500], 'Bedrooms': [3]})
print("Predicted Price for Area=1500 & Bedrooms=3:", model.predict(sample)[0])

=============================================================================================================================================


 The employee’s aptitude and job proficiency score is as follows.

import pandas as pd
from scipy.stats import chi2_contingency

# Step 1: Create the contingency table
data = [[10, 20, 10],   # Low Aptitude
        [20, 30, 20],   # Medium Aptitude
        [20, 30, 40]]   # High Aptitude

# Step 2: Convert it into a pandas DataFrame for readability
df = pd.DataFrame(data, columns=["Low Proficiency", "Medium Proficiency", "High Proficiency"],
                         index=["Low Aptitude", "Medium Aptitude", "High Aptitude"])

print("Contingency Table:")
print(df)

# Step 3: Perform Chi-Square Test
chi2, p, dof, expected = chi2_contingency(df)

# Step 4: Display the results
print(f"\nChi-Square Value: {chi2:.2f}")
print(f"Degrees of Freedom: {dof}")
print(f"P-Value: {p:.4f}")
print("\nExpected Frequencies:")
print(pd.DataFrame(expected, columns=df.columns, index=df.index))

# Step 5: Interpret the result
alpha = 0.05
if p < alpha:
    print("\nConclusion: Reject the null hypothesis — There is a significant association between aptitude and job proficiency.")
else:
    print("\nConclusion: Fail to reject the null hypothesis — No significant association between aptitude and job proficiency.")


--------------------------------------------------------------------------------------------------------------------------------------------


